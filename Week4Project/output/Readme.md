---
title: "Readme"
author: "Whitchurch Muthumani"
date: "1/25/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction


**Important information:** *To view a cleaner version of this file open Readme.pdf*

This document (Readme.Rmd) will guide the reader through the various sections that comprise this week's assignment.
It will explain the context of the data; the script to analyze the data; the resultant dataset generated;
justify why the resultant dataset is tidy. Finally, it will provide a script to view the tidydata which was generated.

This document is broken up into 4 sections

- Section 1 : Context of the data.
- Section 2 : File types in the project.
- Section 3 : Walkthrough of the run_Analysis.r script.
- Section 4 : Justification as to why the tidydata.txt is Tidy.
- Section 5 : Script to view the tidy data: tidydata.txt
- Explanation for DataFrameCreator function



## Section 1 : Context of the data. 
The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. 

The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.[^1]


## Section 2: File types in the project.
### Sources of data:
| File | Description |
|------:|:-----|
| features_info.txt|Shows information about the variables used on the feature vector.|
| features.txt|List of all features.|
| activity_labels.txt|Links the class labels with their activity name.|
| X_train.txt|Training set.|
| y_train.txt|Training labels.|
| X_test.txt|Test set|
| y_test.txt|Test labels.|

### Analysis Script file
This is the script, that takes the tables listed in "Sources of data" and performs analysis on them to generate the tidy data 
output file.

| File | Description |
|------:|:-----|
|run_analysis.R|Script to generate the tidy data|

### Output file
This is the final output generated by the run_analysis.R script. 
The file generated is names: tidydata.txt.

| File | Description |
|------:|:-----|
|tidydata.txt|This file contains the tidydata|

```{r}

```


### Codebook file
This file contains the description of the variables, that make up the columns of the output tidy data set: tidydata.txt.
It also contains the units for each of the variables as applicable
It contains a description of the variables.


## Section 2: Walkthrough of the run_Analysis.r script. 

**Note to Grader: **
**Location of file: ** **Week4Project/scripts/run_analysis.R **

#### Step 0: Clear the environment variables and load the dplyr package
```{r}
rm(list = ls())
library(dplyr)
```

This is to ensure that , we have a clean environment, before beginning the execution of our code.

#### Step 1: Execute the DataFrameCreator function
This is to initialize the DataFrameCreator function. The function will be explained in greater depth below:
```{r echo=FALSE}
DataFrameCreator <- function(path1,path2,path3,features){
  
  # ======================================Load the Training Data Set==================================#
  pathtolaad <- path1
  traininputDF <- read.csv(pathtolaad,header = FALSE,sep = "")
  
  #Get an idea about the dataFrame structure
  head(traininputDF)
  nrow(traininputDF)
  ncol(traininputDF)
  
  # ======================================Load the Activity Dataset==================================#
  pathtolaad <- path2
  trainoutputDF <- read.csv(pathtolaad,header = FALSE,sep ="")
  
  
  # ======================================Load the Volunteer Dataset==================================#
  pathtolaad <- path3
  subjecttDF <- read.csv(pathtolaad,header = FALSE,sep ="")
  
  
  # ==============================Assign appropriate Variable names to Dataframe columns==================================#
  
  
  subjecttDF <- subjecttDF%>%rename("volunteer_id" = names(subjecttDF))
  trainoutputDF <- trainoutputDF%>%rename("activities" = names(trainoutputDF))
  
  
  #============================Load the 561 feature vector names from the features.txt file=================
  pathtolaad <- features
  featureVariableNamesDF <- read.csv(pathtolaad,header = FALSE,sep="\\")

  
  #Inorder to remove the numerics and perform string opertaions we convert it to a character type
  featureVariableNamesDF$V1 <- as.character(featureVariableNamesDF$V1)
  resultantSplit <- strsplit(featureVariableNamesDF$V1," ")
  featureVariableNameVector <- sapply(resultantSplit, function(x) x[-1]) # we got back a vector with all the featurenames
  
  #Finally we apply the variable names to the columns of the appropriate data set
  names(traininputDF)[1:ncol(traininputDF)] <- featureVariableNameVector[1:length(featureVariableNameVector)]
  
  #=============================Finally combine all 3 dataframes to create the Training Data Set =====#
  
  FinalTrainingDF <- cbind(subjecttDF,trainoutputDF,traininputDF)
  
  
}
```





#### Step 2:Create the Training Dataset by calling the function DataFrameCreator
```{r}
pathofinterest <- getwd()
pathtolaad1 <- gsub("Week4Project/output","/Week4Project/data/dataset/train/X_train.txt",pathofinterest)
pathtolaad2 <- gsub("Week4Project/output","/Week4Project/data/dataset/train/y_train.txt",pathofinterest)
pathtolaad3 <- gsub("Week4Project/output","/Week4Project/data/dataset/train/subject_train.txt",pathofinterest)
featurestoload <- gsub("Week4Project/output","/Week4Project/data/dataset/features.txt",pathofinterest)

#Call DataFrameCreator function to get the Trained Dataset 
TrainDataSet <- DataFrameCreator(path1 = pathtolaad1,path2 = pathtolaad2,path3 = pathtolaad3,features = featurestoload)
```

We will not look at the top 5 rows of the TrainDataSet:(Number of columns displayed have been cut down for the sake of brevity)
```{r echo=FALSE}
head(TrainDataSet[1:5], n = 5)
a=nrow(TrainDataSet)
b= ncol(TrainDataSet)

sprintf("The dimensions of the TrainDataSet: Rows = %i ; Columns = %i;", a, b)
```
#### Step 3:Create the Test Dataset by calling the function DataFrameCreator
```{r}
pathofinterest <- getwd()
pathtolaad1 <- gsub("Week4Project/output","/Week4Project/data/dataset/test/X_test.txt",pathofinterest)
pathtolaad2 <- gsub("Week4Project/output","/Week4Project/data/dataset/test/y_test.txt",pathofinterest)
pathtolaad3 <- gsub("Week4Project/output","/Week4Project/data/dataset/test/subject_test.txt",pathofinterest)
featurestoload <- gsub("Week4Project/output","/Week4Project/data/dataset/features.txt",pathofinterest)

#Call DataFrameCreator function to get the Trained Dataset 
TestDataSet <- DataFrameCreator(path1 = pathtolaad1,path2 = pathtolaad2,path3 = pathtolaad3,features = featurestoload)
```

We will not look at the top 5 rows of the TestDataSet:(Number of columns displayed have been cut down for the sake of brevity)
```{r echo=FALSE}
head(TestDataSet[1:5], n = 5)
a=nrow(TestDataSet)
b= ncol(TestDataSet)

sprintf("The dimensions of the TestDataSet: Rows = %i ; Columns = %i;", a, b)
```


#### Step 4: Create the Merged Data Set; Merging Training and Test Datasets
```{r}
MergedDataSet <- rbind(TrainDataSet,TestDataSet)
a=nrow(MergedDataSet)
b= ncol(MergedDataSet)

sprintf("The dimensions of the MergedDataSet: Rows = %i ; Columns = %i;", a, b)
```
#### Step 5: Extract the mean and SD columns:-
```{r}
str(v1 <- grep("std|mean",names(MergedDataSet)))
ColumnsToSelect <- names(MergedDataSet[v1])
print(ColumnsToSelect)
```

**Note:**  *I have included MeafFrequency, as it calculates Mean of Frequencies. This was done willfully and not as an oversight.*
*There has been debate about whether MEanFrequency should be included or not. I decided to Err in the side of caution by including it.*
*As it is better to have more variables, than accidently neglect variables which maybe required in the future.*

```{r echo=FALSE}
ColumnsToSelect <- append(ColumnsToSelect,c("volunteer_id","activities"),after = 0)
#str(ColumnsToSelect)
#duplicated(ColumnsToSelect)
```
**Cleaning up of duplicate columns:** *There were duplicate columns the code below exposes those*
```{r echo=FALSE}
duplicateColumnMask <- duplicated(names(MergedDataSet))
duplicatevalues <- sum(as.integer(duplicateColumnMask)) #84 duplicates


v2 <- grep("std|mean",(names(MergedDataSet[duplicated(names(MergedDataSet))])))
ColumnsToSelectV2 <- names(MergedDataSet[v2]) # returns 0, This means none of the duplicated columns are of interest to us
```

```{r echo=FALSE}
sprintf("Total Number of Duplicate Columns = %i", duplicatevalues)
sprintf("Are there any mean std columns that are duplicated= %i", sum(as.integer(ColumnsToSelectV2)))
```

*There are no columns of interests which have been duplicated, we can proceed by just dropping the duplicate columns*
*Therefore instead of wrangling the data further, I take only the columns of interest, ignore the rest.*
```{r echo=FALSE}
MergedDataSet <- MergedDataSet[,ColumnsToSelect]
#str(MergedDataSet)
```

#### Step 6: Use descriptive activity names for activities in the dataset
**Before** *Converting activityID to Activity Names*
```{r echo=FALSE}
print(head(MergedDataSet$activities,n=5))
```

**Applying the code to convert ActivityID to Activity Names **
```{r}
pathofinterest <- getwd()
pathtoload <- gsub("Week4Project/output","Week4Project/data/dataset/activity_labels.txt",pathofinterest)
#pathtoload <- "./Week4Project/data/dataset/activity_labels.txt"
activityDF <- read.csv(pathtoload,header = FALSE,sep=" ")
activityDF <- activityDF%>%rename("activity_id" = "V1")
activityDF <- activityDF%>%rename("activity" = "V2")

#head(MergedDataSet$activities)
#head(activityDF)

#res <- vector(mode="character",length = length(MergedDataSet$activities))

for(X in activityDF$activity_id)
{
  if(grep(X,activityDF$activity_id))
  {
    #print(X)
    #print(as.character(activityDF[X,"activity"]))
    MergedDataSet$activities <- gsub(X,as.character(activityDF[X,"activity"]),MergedDataSet$activities)
   
    
  }
}
```

**End Result** *Activity Names should apprear in the output *
```{r echo=FALSE}
print(head(MergedDataSet$activities,n=5))
```

#### Step 7: Give more descriptive names for the variables so non-domain experts understand it
*This is achieved by grepping for regular Expression patterns and then subbing them with more verbose wording*
*I used to approach to generate, long verbose variable names, which are easy to understand but large in length*
*The code below demonstrates the grep and sub pattern of string replacement*
```{r}
names(MergedDataSet) <- gsub("-","",names(MergedDataSet))
names(MergedDataSet) <- gsub("X$","_AlongXAxis",names(MergedDataSet))
names(MergedDataSet) <- gsub("Y$","_AlongYAxis",names(MergedDataSet))
names(MergedDataSet) <- gsub("Z$","_AlongZAxis",names(MergedDataSet))
names(MergedDataSet) <- gsub("std()","_StandardDeviation",names(MergedDataSet))
names(MergedDataSet) <- gsub("meanFreq()","_Meanfrequency",names(MergedDataSet))
names(MergedDataSet) <- gsub("[()]","",names(MergedDataSet))
names(MergedDataSet) <- gsub("mean","_Mean",names(MergedDataSet))
names(MergedDataSet) <- gsub("Acc","Acceleration",names(MergedDataSet))
names(MergedDataSet) <- gsub("Mag","Magnitude",names(MergedDataSet))
names(MergedDataSet) <- gsub("Gyro","AngularVelocity",names(MergedDataSet))
names(MergedDataSet) <- gsub("^t","Time",names(MergedDataSet))
names(MergedDataSet) <- gsub("^f","Frequency",names(MergedDataSet))
names(MergedDataSet) <- gsub("Jerk","JerkSignal",names(MergedDataSet))
names(MergedDataSet) <- gsub("BodyBody","Body",names(MergedDataSet))
```

#### Step 8: Creating the tidy dataset 
```{r}
# We group by Volunteer_id ad activities.

MergedDataSet <- group_by(MergedDataSet,volunteer_id)
MergedDataSet <- group_by(MergedDataSet,activities,add = TRUE)


#Now if we apply summarize_at, with means as the function to apply. it will summarize the columns of interest,
#by breaking them into sections. The sections are demarcated by (Volunteer_id and Activity) which is what group_by
#did for us.

vec <- names(MergedDataSet)
vec <- tail(vec,-2)
str(vec)

tidySet <- summarize_at(MergedDataSet,.vars = vec ,.funs = mean)

```

#### Step 9: View the tidydata
```{r}
knitr::kable(tidySet[1:3], caption = "The Tidy Data Set")
sprintf("The tidy dataset dimensions are; rows = %i, columns = %i",nrow(tidySet),ncol(tidySet))
```
*I am only printing the first 4 columns as, the table is too large to fit all columns inside the page*
*I have printed the dimensions so that the grader can check that the number of rows and columns are correct*

#### Step 10: Writing the tidydata to a file caled tidydata.txt
```{r}
pathofinterest <- getwd()
pathtolaad <- gsub("Week4Project/output","Week4Project/output/tidydata.txt",pathofinterest)
write.csv(tidySet,pathtolaad,row.names = FALSE)

```
## Section 4 : Justification as to why the tidydata.txt is Tidy.
**Note :** To faciitate ease of displaying the table in this section, i will be making the columnnames shorter
```{r}
tidysetcopy <- tidySet
names(tidysetcopy)<- abbreviate(names(tidysetcopy), minlength=2)
knitr::kable(tidysetcopy[1:10,1:5], caption = "Abbreviated column names for ease of display")
sprintf("The tidy dataset dimensions are; rows = %i, columns = %i",nrow(tidySet),ncol(tidySet))

```


Inorder to satisfy the Tidy Principle 3 things should be satisfied:-

- I. Each variable forms a column. 
- II. Each observation forms a row.
- III. Each type of observational unit forms a table.

### I. Each variable forms a column:
```{r}
knitr::kable(tidysetcopy[1:2,1:5], caption = "Abbreviated column names for ease of display")
```

In our dataset that has been output. Every one of the variables: 

- TimeBodyAcceleration_Mean_AlongXAxis -> abbreviated as -> TBA_M_AX
- TimeBodyAcceleration_Mean_AlongYAxis -> abbreviated as -> TBA_M_AY
- TimeBodyAcceleration_Mean_AlongZAxis -> abbreviated as - > TBA_M_AZ

- and so on...., has a separate column. Therefore , the 1st rule has been satisfied

### II. Each observation forms a row:
```{r}
knitr::kable(tidysetcopy[10,1:5], caption = "Abbreviated column names for ease of display")
```

By observing a single row of the dataset we see that, every observation, has a single row. And each row contains all the attribute
variables. Therefore the 2nd rule has been satisfied.

### III. Each type of observational unit forms a table:
```{r}
tibblerepresentation <- as_tibble(tidySet)
print(tibblerepresentation)
```

The entire dataframe represents the readings that have been taken from the sensor of the Samsung Galaxy phone. As such it reprsents a
single observational unit of data. Therefore the 3rd rule has been satisfied.

**Therefore this tidy data set form we have achieved after running the script is a valid tidy data representation**


## Section 5 : Script to view the tidy data: tidydata.txt
**Note to Grader :**

- In order to view the entire cleaned up tidy data table.
- Run the script: showTidyData.R (located in the output folder path: Week4Project/output/showTidyData.R)
- This will display the entire tidy data set table

# Explanation for DataFrameCreator function
The DataFrameCreator function has: 3 parameters: path1, path2, path3: for Training, Test and Subject text files respectively
```{r, eval=FALSE}
 DataFrameCreator <- function(path1,path2,path3,features {}
```




The first section of the function is for loading the input Data set that contains all the feature variables, using the path1 variable.

    
```{r,eval=FALSE}
      # ==Load the input Data Set======#
   pathtolaad <- path1
  traininputDF <- read.csv(pathtolaad,header = FALSE,sep = "")
  
  #Get an idea about the dataFrame structure
  head(traininputDF)
  nrow(traininputDF)
  ncol(traininputDF)  
```


The second section of the function is for loading the Activity Data set  that contains activities information using the path 2 variable.
```{r,eval=FALSE}


  # ===Load the output/Activity Dataset====#
  pathtolaad <- path2
  trainoutputDF <- read.csv(pathtolaad,header = FALSE,sep ="")
```


The third section of the function is for loading the subject data set using the path 3 variable
```{r,eval=FALSE}


 # ==Load the Volunteer Dataset====#
  pathtolaad <- path3
  subjecttDF <- read.csv(pathtolaad,header = FALSE,sep ="")
```



The fourth section of the function is for assigning propert neames to the DataFrame columns
```{r,eval=FALSE}
 # ====Assign appropriate Variable names to Dataframe columns===#
  
  
  subjecttDF <- subjecttDF%>%rename("volunteer_id" = names(subjecttDF))
 trainoutputDF <- trainoutputDF%>%rename("activities" = names(trainoutputDF))
```


 The fifth section is for assigning the proper variable names for each column replacing the V1, V2 V3 etc 
 
```{r,eval=FALSE}
  #===Load the 561 feature vector names from the features.txt file==#
  pathtolaad <- features
 featureVariableNamesDF <- read.csv(pathtolaad,header = FALSE,sep="\\")

  
  #Inorder to remove the numerics and perform string opertaions we convert it to a character type
  featureVariableNamesDF$V1 <- as.character(featureVariableNamesDF$V1)
  resultantSplit <- strsplit(featureVariableNamesDF$V1," ")
  featureVariableNameVector <- sapply(resultantSplit, function(x) x[-1]) # we got back a vector with all the featurenames
  
  #Finally we apply the variable names to the columns of the appropriate data set
  names(traininputDF)[1:ncol(traininputDF)] <- featureVariableNameVector[1:length(featureVariableNameVector)]
```
 

 The sixth section is for combining all 3 dataframes: subject, input and output to create the final dataframe  
```{r,eval=FALSE}
  #=Finally combine all 3 dataframes to create the Training Data Set =====#
  
  FinalTrainingDF <- cbind(subjecttDF,trainoutputDF,traininputDF)
```





  

[^1]: Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living (IWAAL 2012). Vitoria-Gasteiz, Spain. Dec 2012









